{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98ad0115-2c2b-4223-8b5c-4d6c17d5260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import gzip\n",
    "from scipy.sparse import coo_matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1ddde9-9837-4e44-9977-a8288fdcc157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEHNN layer\n",
    "class DEHNNLayer(nn.Module):\n",
    "    def __init__(self, node_in_features, edge_in_features):\n",
    "        super(DEHNNLayer, self).__init__()\n",
    "        self.node_mlp1 = nn.Sequential(\n",
    "            nn.Linear(edge_in_features, edge_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.edge_mlp2 = nn.Sequential(\n",
    "            nn.Linear(node_in_features, node_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.edge_mlp3 = nn.Sequential(\n",
    "            nn.Linear(2 * node_in_features, 2 * node_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.node_to_virtual_mlp = nn.Sequential(\n",
    "            nn.Linear(node_in_features, node_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.virtual_to_higher_virtual_mlp = nn.Sequential(\n",
    "            nn.Linear(node_in_features, edge_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.higher_virtual_to_virtual_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_in_features, edge_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.virtual_to_node_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_in_features, edge_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Learnable defaults for missing driver or sink\n",
    "        self.default_driver = nn.Parameter(torch.zeros(node_in_features))\n",
    "        self.default_sink_agg = nn.Parameter(torch.zeros(node_in_features))\n",
    "        self.default_edge_agg = nn.Parameter(torch.zeros(edge_in_features))\n",
    "        self.default_virtual_node = nn.Parameter(torch.zeros(node_in_features))\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize all parameters with Xavier uniform distribution.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, node_features, edge_features, hypergraph):\n",
    "        # Node update\n",
    "        updated_node_features = {}\n",
    "        for node in hypergraph.nodes:\n",
    "            incident_edges = hypergraph.get_incident_edges(node)\n",
    "            if incident_edges:\n",
    "                agg_features = torch.sum(torch.stack([self.node_mlp1(edge_features[edge]) for edge in incident_edges]), dim=0)\n",
    "            else:\n",
    "                agg_features = self.default_edge_agg\n",
    "            updated_node_features[node] = agg_features\n",
    "\n",
    "        # Edge update\n",
    "        updated_edge_features = {}\n",
    "        for edge in hypergraph.edges:\n",
    "            driver, sinks = hypergraph.get_driver_and_sinks(edge)\n",
    "\n",
    "            driver_feature = node_features[driver] if driver is not None else self.default_driver\n",
    "\n",
    "            if sinks:\n",
    "                sink_agg = torch.sum(torch.stack([self.edge_mlp2(node_features[sink]) for sink in sinks]), dim=0)\n",
    "            else:\n",
    "                sink_agg = self.default_sink_agg\n",
    "\n",
    "            concatenated = torch.cat([driver_feature, sink_agg])\n",
    "            updated_edge_features[edge] = self.edge_mlp3(concatenated)\n",
    "\n",
    "        # Virtual node aggregation\n",
    "        virtual_node_agg = {}\n",
    "        for virtual_node in range(hypergraph.num_virtual_nodes):\n",
    "            assigned_nodes = [node for node in hypergraph.nodes if hypergraph.get_virtual_node(node) == virtual_node]\n",
    "            if assigned_nodes:\n",
    "                agg_features = torch.sum(torch.stack([self.node_to_virtual_mlp(node_features[node]) for node in assigned_nodes]), dim=0)\n",
    "            else:\n",
    "                agg_features = self.default_virtual_node\n",
    "            virtual_node_agg[virtual_node] = agg_features\n",
    "\n",
    "        higher_virtual_feature = torch.sum(\n",
    "            torch.stack([self.virtual_to_higher_virtual_mlp(virtual_node_agg[vn]) for vn in virtual_node_agg]), dim=0\n",
    "        )\n",
    "\n",
    "        propagated_virtual_node_features = {}\n",
    "        for virtual_node in range(hypergraph.num_virtual_nodes):\n",
    "            propagated_virtual_node_features[virtual_node] = self.higher_virtual_to_virtual_mlp(higher_virtual_feature)\n",
    "\n",
    "        for node in hypergraph.nodes:\n",
    "            virtual_node = hypergraph.get_virtual_node(node)\n",
    "            propagated_feature = self.virtual_to_node_mlp(propagated_virtual_node_features[virtual_node])\n",
    "            updated_node_features[node] += propagated_feature\n",
    "\n",
    "        return updated_node_features, updated_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c310407-0a0e-41ea-9d1d-526781a7613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEHNN model\n",
    "class DEHNN(nn.Module):\n",
    "    def __init__(self, num_layers, node_in_features, edge_in_features):\n",
    "        super(DEHNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(DEHNNLayer(node_in_features, edge_in_features))\n",
    "            node_in_features, edge_in_features = edge_in_features, node_in_features\n",
    "            edge_in_features *= 2\n",
    "\n",
    "        edge_in_features = edge_in_features // 2\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(node_in_features, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, node_features, edge_features, hypergraph):\n",
    "        for layer in self.layers:\n",
    "            node_features, edge_features = layer(node_features, edge_features, hypergraph)\n",
    "        \n",
    "        final_node_features = torch.stack([node_features[node] for node in hypergraph.nodes], dim=0)\n",
    "        output = self.output_layer(final_node_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2923ee28-7061-4730-b110-362db18c2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Hypergraph Implementation\n",
    "class Hypergraph:\n",
    "    def __init__(self, nodes, edges, driver_sink_map, node_to_virtual_map, num_virtual_nodes):\n",
    "        self.nodes = nodes\n",
    "        self.edges = edges\n",
    "        self.driver_sink_map = driver_sink_map\n",
    "        self.node_to_virtual_map = node_to_virtual_map\n",
    "        self.num_virtual_nodes = num_virtual_nodes\n",
    "\n",
    "    def get_incident_edges(self, node):\n",
    "        return [edge for edge in self.edges if node in self.driver_sink_map[edge][1] or node == self.driver_sink_map[edge][0]]\n",
    "\n",
    "    def get_driver_and_sinks(self, edge):\n",
    "        return self.driver_sink_map[edge]\n",
    "    \n",
    "    def get_virtual_node(self, node):\n",
    "        return self.node_to_virtual_map[node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd023cb-be14-434f-8737-3852b058a8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Epoch [1/10], Loss: 3.7890\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Epoch [2/10], Loss: 2.4258\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Epoch [3/10], Loss: 3.7727\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Epoch [4/10], Loss: 3.3356\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Epoch [5/10], Loss: 2.1742\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Epoch [6/10], Loss: 1.7811\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Epoch [7/10], Loss: 2.3288\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Epoch [8/10], Loss: 1.9070\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Epoch [9/10], Loss: 1.4795\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "Epoch [10/10], Loss: 1.7770\n"
     ]
    }
   ],
   "source": [
    "file_indices = range(1, 9)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DEHNN(num_layers=2, node_in_features=14, edge_in_features=1).to(device)\n",
    "\n",
    "# Training configuration\n",
    "epochs = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0  # Accumulate loss over all datasets for each epoch\n",
    "    \n",
    "    for i in file_indices:\n",
    "        print(i)\n",
    "        # Load data for the current file\n",
    "        clean_data_dir = '../data/processed_data/'\n",
    "        \n",
    "        with open(f'{clean_data_dir}{i}.driver_sink_map.pkl', 'rb') as f:\n",
    "            driver_sink_map = pickle.load(f)\n",
    "        \n",
    "        with open(f'{clean_data_dir}{i}.node_features.pkl', 'rb') as f:\n",
    "            node_features = pickle.load(f)\n",
    "        \n",
    "        with open(f'{clean_data_dir}{i}.net_features.pkl', 'rb') as f:\n",
    "            edge_features = pickle.load(f)\n",
    "        \n",
    "        with open(f'{clean_data_dir}{i}.congestion.pkl', 'rb') as f:\n",
    "            congestion = pickle.load(f)\n",
    "        \n",
    "        partition = np.load(f'{clean_data_dir}{i}.partition.npy')\n",
    "        \n",
    "        # Preprocesses the data\n",
    "        node_features = {k: torch.tensor(v).float().to(device) for k, v in node_features.items()}\n",
    "        edge_features = {k: torch.tensor(v).float().to(device) for k, v in edge_features.items()}\n",
    "        \n",
    "        nodes = list(range(len(node_features)))\n",
    "        edges = list(range(len(edge_features)))\n",
    "        hypergraph = Hypergraph(nodes, edges, driver_sink_map, partition, 2)\n",
    "        \n",
    "        # Performs forward pass\n",
    "        output = model(node_features, edge_features, hypergraph)\n",
    "        \n",
    "        # Dummy target for illustration (binary labels for each node: 0 for not congested, 1 for congested)\n",
    "        target = torch.tensor(list(congestion.values())).to(device)\n",
    "        \n",
    "        # Computes the loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Performs backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()  # Reset gradients after each batch\n",
    "        \n",
    "        # Sum up the  loss\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Print the epoch loss\n",
    "    print(f'Epoch [{epoch+1}/10], Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5948f-fc53-4e2b-8b0d-7ee45a029f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
