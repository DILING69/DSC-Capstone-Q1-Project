{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import gzip\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.stats import binned_statistic_2d\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import pymetis\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from numpy.linalg import eigvals\n",
    "from torch_geometric.utils import (get_laplacian, to_scipy_sparse_matrix, to_undirected, to_dense_adj)\n",
    "from scipy.sparse.linalg import eigsh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = '../../DigIC_dataset/'\n",
    "clean_data_dir = 'processed_data/'\n",
    "design = 'xbar'\n",
    "n_variants = 13\n",
    "\n",
    "sample_names = []\n",
    "corresponding_design = []\n",
    "corresponding_variant = []\n",
    "for idx in range(n_variants):\n",
    "    sample_name = raw_data_dir + design + '/' + str(idx + 1) + '/'\n",
    "    sample_names.append(sample_name)\n",
    "    corresponding_design.append(design)\n",
    "    corresponding_variant.append(idx + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Data based on Width and Height and Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_fn = raw_data_dir + 'cells.json.gz'\n",
    "with gzip.open(cells_fn, 'r') as fin:\n",
    "    cell_data = json.load(fin)\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "for idx in range(len(cell_data)):\n",
    "    width = cell_data[idx]['width']\n",
    "    height = cell_data[idx]['height']\n",
    "    widths.append(width)\n",
    "    heights.append(height)\n",
    "\n",
    "widths = np.array(widths)\n",
    "heights = np.array(heights)\n",
    "\n",
    "min_cell_width = np.min(widths)\n",
    "max_cell_width = np.max(widths)\n",
    "min_cell_height = np.min(heights)\n",
    "max_cell_height = np.max(heights)\n",
    "\n",
    "widths = (widths - min_cell_width) / (max_cell_width - min_cell_width)\n",
    "heights = (heights - min_cell_height) / (max_cell_height - min_cell_height)\n",
    "\n",
    "# For each cell map the input and output pins\n",
    "cell_to_edge_dict = {item['id']:{inner_item['id']: inner_item['dir'] for inner_item in item['terms']} for item in cell_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Features from Each Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing data!\n"
     ]
    }
   ],
   "source": [
    "for sample in range(n_variants):\n",
    "    folder = sample_names[sample]\n",
    "    design = corresponding_design[sample]\n",
    "    instances_nets_fn = folder + design + '.json.gz'\n",
    "\n",
    "    with gzip.open(instances_nets_fn, 'r') as fin:\n",
    "        instances_nets_data = json.load(fin)\n",
    "\n",
    "    instances = instances_nets_data['instances']\n",
    "    nets = instances_nets_data['nets']\n",
    "\n",
    "    inst_to_cell = {item['id']:item['cell'] for item in instances}\n",
    "\n",
    "    num_instances = len(instances)\n",
    "    num_nets = len(nets)\n",
    "\n",
    "\n",
    "    xloc_list = [instances[idx]['xloc'] for idx in range(num_instances)]\n",
    "    yloc_list = [instances[idx]['yloc'] for idx in range(num_instances)]\n",
    "    cell = [instances[idx]['cell'] for idx in range(num_instances)]\n",
    "    cell_width = [widths[cell[idx]] for idx in range(num_instances)]\n",
    "    cell_height = [heights[cell[idx]] for idx in range(num_instances)]\n",
    "    orient = [instances[idx]['orient'] for idx in range(num_instances)]\n",
    "    \n",
    "    x_min = min(xloc_list)\n",
    "    x_max = max(xloc_list)\n",
    "    y_min = min(yloc_list)\n",
    "    y_max = max(yloc_list)\n",
    "\n",
    "    X = np.expand_dims(np.array(xloc_list), axis = 1)\n",
    "    Y = np.expand_dims(np.array(yloc_list), axis = 1)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "    Y = (Y - y_min) / (y_max - y_min)\n",
    "\n",
    "    cell = np.expand_dims(np.array(cell), axis = 1)\n",
    "    cell_width = np.expand_dims(np.array(cell_width), axis = 1)\n",
    "    cell_height = np.expand_dims(np.array(cell_height), axis = 1)\n",
    "    orient = np.expand_dims(np.array(orient), axis = 1)\n",
    "\n",
    "    instance_features = np.concatenate((X, Y, cell, cell_width, cell_height, orient), axis = 1)\n",
    "\n",
    "    \n",
    "    # Processing Connectivity Data\n",
    "    \n",
    "    connection_fn = folder + design + '_connectivity.npz'\n",
    "    connection_data = np.load(connection_fn)\n",
    "    \n",
    "    # Find direction of edge in instance and netlist\n",
    "    dirs = []\n",
    "    edge_t = connection_data['data']\n",
    "    instance_idx = connection_data['row']\n",
    "    \n",
    "    for idx in range(len(instance_idx)):\n",
    "        inst = instance_idx[idx]\n",
    "        cell = inst_to_cell[inst]\n",
    "        edge_dict = cell_to_edge_dict[cell]\n",
    "        t = edge_t[idx]\n",
    "        direction = edge_dict[t]\n",
    "        dirs.append(direction)\n",
    "\n",
    "    dirs = np.array(dirs)\n",
    "\n",
    "    # Build Driver Sink Mapping\n",
    "    driver_sink_map = defaultdict(lambda: (None, []))\n",
    "\n",
    "    # Extract unique nodes and edges\n",
    "    nodes = list(set(connection_data['row']))\n",
    "    edges = list(set(connection_data['col']))\n",
    "\n",
    "    # Populate driver_sink_map\n",
    "    for node, edge, direction in zip(connection_data['row'], connection_data['col'], dirs):\n",
    "        if direction == 1:\n",
    "            driver_sink_map[edge] = (node, driver_sink_map[edge][1])\n",
    "        elif direction == 0:\n",
    "            driver_sink_map[edge][1].append(node)\n",
    "\n",
    "    driver_sink_map = dict(driver_sink_map)\n",
    "    \n",
    "    # Compute Net Features\n",
    "    net_features = {}\n",
    "    for k, v in driver_sink_map.items():\n",
    "        if v[0]:\n",
    "            net_features[k] = [len(v[1]) + 1]\n",
    "        else:\n",
    "            net_features[k] = [len(v[1])]\n",
    "\n",
    "    instance_idx = connection_data['row']\n",
    "    net_idx = connection_data['col']\n",
    "    net_idx += num_instances\n",
    "\n",
    "    v1 = torch.unsqueeze(torch.Tensor(np.concatenate([instance_idx, net_idx], axis = 0)).long(), dim = 1)\n",
    "    v2 = torch.unsqueeze(torch.Tensor(np.concatenate([net_idx, instance_idx], axis = 0)).long(), dim = 1)\n",
    "    \n",
    "    # Create Graph Representation\n",
    "    undir_edge_index = torch.transpose(torch.cat([v1, v2], dim = 1), 0, 1)\n",
    "\n",
    "    L = to_scipy_sparse_matrix(\n",
    "        *get_laplacian(undir_edge_index, normalization = \"sym\", num_nodes = num_instances + num_nets)\n",
    "    )\n",
    "    evals, evects = eigsh(L, k = 10, which='SM')\n",
    "\n",
    "    node_features = {}\n",
    "    for i in range(num_instances):\n",
    "        node_features[i] = np.concatenate([instance_features[i, 2:], evects[i]])\n",
    "\n",
    "        \n",
    "    # Create Congestion data\n",
    "    \n",
    "    congestion_fn = folder + design + '_congestion.npz'\n",
    "    congestion_data = np.load(congestion_fn)\n",
    "\n",
    "    congestion_data_demand = congestion_data['demand']\n",
    "    congestion_data_capacity = congestion_data['capacity']\n",
    "\n",
    "    num_layers = len(list(congestion_data['layerList']))\n",
    "\n",
    "    ybl = congestion_data['yBoundaryList']\n",
    "    xbl = congestion_data['xBoundaryList']\n",
    "\n",
    "    all_demand = []\n",
    "    all_capacity = []\n",
    "\n",
    "    # Going through each layer\n",
    "    \n",
    "    for layer in list(congestion_data['layerList']):\n",
    "        lyr = list(congestion_data['layerList']).index(layer)\n",
    "        ret = binned_statistic_2d(xloc_list, yloc_list, None, 'count', bins = [xbl[1:], ybl[1:]], expand_binnumbers = True)\n",
    "\n",
    "        i_list = np.array([ret.binnumber[0, idx] - 1 for idx in range(num_instances)])\n",
    "        j_list = np.array([ret.binnumber[1, idx] - 1 for idx in range(num_instances)])\n",
    "\n",
    "        # Getting demand and capacity\n",
    "        demand_list = np.array(congestion_data_demand[lyr, i_list, j_list].flatten())\n",
    "        capacity_list = np.array(congestion_data_capacity[lyr, i_list, j_list].flatten())\n",
    "\n",
    "        all_demand.append(np.expand_dims(demand_list, axis = 1))\n",
    "        all_capacity.append(np.expand_dims(capacity_list, axis = 1))\n",
    "\n",
    "        average_demand = np.mean(demand_list)\n",
    "        average_capacity = np.mean(capacity_list)\n",
    "        average_diff = np.mean(capacity_list - demand_list)\n",
    "        count_congestions = np.sum(demand_list > capacity_list)\n",
    "\n",
    "    demand = np.concatenate(all_demand, axis = 1).sum(axis=1)\n",
    "    capacity = np.concatenate(all_capacity, axis = 1).sum(axis=1)\n",
    "\n",
    "    congestion_actual = {}\n",
    "    \n",
    "    # Create pickle files for all features/mapping/data\n",
    "    for i in range(len(node_features)):\n",
    "        congestion_actual[i] = int(((capacity[i] * 0.9) - demand[i]) < 0)\n",
    "\n",
    "    with open(f'{clean_data_dir}{sample+1}.driver_sink_map.pkl', 'wb') as f:\n",
    "        pickle.dump(driver_sink_map, f)\n",
    "    \n",
    "    with open(f'{clean_data_dir}{sample+1}.node_features.pkl', 'wb') as f:\n",
    "        pickle.dump(node_features, f)\n",
    "\n",
    "    with open(f'{clean_data_dir}{sample+1}.net_features.pkl', 'wb') as f:\n",
    "        pickle.dump(net_features, f)\n",
    "\n",
    "    with open(f'{clean_data_dir}{sample+1}.congestion.pkl', 'wb') as f:\n",
    "        pickle.dump(congestion_actual, f)\n",
    "print('Done processing data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done adding Partitions!\n"
     ]
    }
   ],
   "source": [
    "# Adding partitions for binary classification\n",
    "\n",
    "n_variants = 13\n",
    "num_partitions = 2\n",
    "\n",
    "for i in range(1, n_variants+1):\n",
    "    connection_data = np.load(f'{raw_data_dir}xbar/{i}/xbar_connectivity.npz')\n",
    "\n",
    "    # Nodes and Hyperedges count\n",
    "    num_nodes = max(connection_data['row']) + 1\n",
    "    num_nets = max(connection_data['col']) + 1\n",
    "\n",
    "    # Creating an adjacency list for Metis\n",
    "    # Change hypergraph to bipartite graph representation\n",
    "    adj_list = [[] for _ in range(num_nodes + num_nets)]\n",
    "    for node, net in zip(connection_data['row'], connection_data['col']):\n",
    "        adj_list[node].append(num_nodes + net)  # Connect node to net\n",
    "        adj_list[num_nodes + net].append(node)  # Connect net to node\n",
    "\n",
    "    cuts, membership = pymetis.part_graph(num_partitions, adjacency=adj_list)\n",
    "    arr = np.array(membership[:num_nodes])\n",
    "\n",
    "    np.save(f'{clean_data_dir}{i}.partition.npy', arr)\n",
    "print('Done adding Partitions!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
