{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ad0115-2c2b-4223-8b5c-4d6c17d5260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import gzip\n",
    "from scipy.sparse import coo_matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c1ddde9-9837-4e44-9977-a8288fdcc157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEHNN layer\n",
    "class DEHNNLayer(nn.Module):\n",
    "    def __init__(self, node_in_features, edge_in_features):\n",
    "        super(DEHNNLayer, self).__init__()\n",
    "        self.node_mlp1 = nn.Sequential(\n",
    "            nn.Linear(edge_in_features, edge_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.edge_mlp2 = nn.Sequential(\n",
    "            nn.Linear(node_in_features, node_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.edge_mlp3 = nn.Sequential(\n",
    "            nn.Linear(2 * node_in_features, 2 * node_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.node_to_virtual_mlp = nn.Sequential(\n",
    "            nn.Linear(node_in_features, node_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.virtual_to_higher_virtual_mlp = nn.Sequential(\n",
    "            nn.Linear(node_in_features, edge_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.higher_virtual_to_virtual_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_in_features, edge_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.virtual_to_node_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_in_features, edge_in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Learnable defaults for missing driver or sink\n",
    "        self.default_driver = nn.Parameter(torch.zeros(node_in_features))\n",
    "        self.default_sink_agg = nn.Parameter(torch.zeros(node_in_features))\n",
    "        self.default_edge_agg = nn.Parameter(torch.zeros(edge_in_features))\n",
    "        self.default_virtual_node = nn.Parameter(torch.zeros(node_in_features))\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize all parameters with Xavier uniform distribution.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, node_features, edge_features, hypergraph):\n",
    "        # Node update\n",
    "        updated_node_features = {}\n",
    "        for node in hypergraph.nodes:\n",
    "            incident_edges = hypergraph.get_incident_edges(node)\n",
    "            if incident_edges:\n",
    "                agg_features = torch.sum(torch.stack([self.node_mlp1(edge_features[edge]) for edge in incident_edges]), dim=0)\n",
    "            else:\n",
    "                agg_features = self.default_edge_agg\n",
    "            updated_node_features[node] = agg_features\n",
    "\n",
    "        # Edge update\n",
    "        updated_edge_features = {}\n",
    "        for edge in hypergraph.edges:\n",
    "            driver, sinks = hypergraph.get_driver_and_sinks(edge)\n",
    "\n",
    "            driver_feature = node_features[driver] if driver is not None else self.default_driver\n",
    "\n",
    "            if sinks:\n",
    "                sink_agg = torch.sum(torch.stack([self.edge_mlp2(node_features[sink]) for sink in sinks]), dim=0)\n",
    "            else:\n",
    "                sink_agg = self.default_sink_agg\n",
    "\n",
    "            concatenated = torch.cat([driver_feature, sink_agg])\n",
    "            updated_edge_features[edge] = self.edge_mlp3(concatenated)\n",
    "\n",
    "        # Virtual node aggregation\n",
    "        virtual_node_agg = {}\n",
    "        for virtual_node in range(hypergraph.num_virtual_nodes):\n",
    "            assigned_nodes = [node for node in hypergraph.nodes if hypergraph.get_virtual_node(node) == virtual_node]\n",
    "            if assigned_nodes:\n",
    "                agg_features = torch.sum(torch.stack([self.node_to_virtual_mlp(node_features[node]) for node in assigned_nodes]), dim=0)\n",
    "            else:\n",
    "                agg_features = self.default_virtual_node\n",
    "            virtual_node_agg[virtual_node] = agg_features\n",
    "\n",
    "        higher_virtual_feature = torch.sum(\n",
    "            torch.stack([self.virtual_to_higher_virtual_mlp(virtual_node_agg[vn]) for vn in virtual_node_agg]), dim=0\n",
    "        )\n",
    "\n",
    "        propagated_virtual_node_features = {}\n",
    "        for virtual_node in range(hypergraph.num_virtual_nodes):\n",
    "            propagated_virtual_node_features[virtual_node] = self.higher_virtual_to_virtual_mlp(higher_virtual_feature)\n",
    "\n",
    "        for node in hypergraph.nodes:\n",
    "            virtual_node = hypergraph.get_virtual_node(node)\n",
    "            propagated_feature = self.virtual_to_node_mlp(propagated_virtual_node_features[virtual_node])\n",
    "            updated_node_features[node] += propagated_feature\n",
    "\n",
    "        return updated_node_features, updated_edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c310407-0a0e-41ea-9d1d-526781a7613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEHNN model\n",
    "class DEHNN(nn.Module):\n",
    "    def __init__(self, num_layers, node_in_features, edge_in_features):\n",
    "        super(DEHNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(DEHNNLayer(node_in_features, edge_in_features))\n",
    "            node_in_features, edge_in_features = edge_in_features, node_in_features\n",
    "            edge_in_features *= 2\n",
    "\n",
    "        edge_in_features = edge_in_features // 2\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(node_in_features, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, node_features, edge_features, hypergraph):\n",
    "        for layer in self.layers:\n",
    "            node_features, edge_features = layer(node_features, edge_features, hypergraph)\n",
    "        \n",
    "        final_node_features = torch.stack([node_features[node] for node in hypergraph.nodes], dim=0)\n",
    "        output = self.output_layer(final_node_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2923ee28-7061-4730-b110-362db18c2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Hypergraph Implementation\n",
    "class Hypergraph:\n",
    "    def __init__(self, nodes, edges, driver_sink_map, node_to_virtual_map, num_virtual_nodes):\n",
    "        self.nodes = nodes\n",
    "        self.edges = edges\n",
    "        self.driver_sink_map = driver_sink_map\n",
    "        self.node_to_virtual_map = node_to_virtual_map\n",
    "        self.num_virtual_nodes = num_virtual_nodes\n",
    "\n",
    "    def get_incident_edges(self, node):\n",
    "        return [edge for edge in self.edges if node in self.driver_sink_map[edge][1] or node == self.driver_sink_map[edge][0]]\n",
    "\n",
    "    def get_driver_and_sinks(self, edge):\n",
    "        return self.driver_sink_map[edge]\n",
    "    \n",
    "    def get_virtual_node(self, node):\n",
    "        return self.node_to_virtual_map[node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd023cb-be14-434f-8737-3852b058a8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'clean_data/1.driver_sink_map.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load data for the current file\u001b[39;00m\n\u001b[1;32m     20\u001b[0m clean_data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_data/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mclean_data_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.driver_sink_map.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     23\u001b[0m     driver_sink_map \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_data_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.node_features.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/miniforge3/envs/DE-HNN/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'clean_data/1.driver_sink_map.pkl'"
     ]
    }
   ],
   "source": [
    "file_indices = range(1, 9)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DEHNN(num_layers=2, node_in_features=14, edge_in_features=1).to(device)\n",
    "\n",
    "# Training configuration\n",
    "epochs = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0  # Accumulate loss over all datasets for each epoch\n",
    "    \n",
    "    for i in file_indices:\n",
    "        print(i)\n",
    "        # Load data for the current file\n",
    "        clean_data_dir = 'clean_data/'\n",
    "        \n",
    "        with open(f'{clean_data_dir}{i}.driver_sink_map.pkl', 'rb') as f:\n",
    "            driver_sink_map = pickle.load(f)\n",
    "        \n",
    "        with open(f'{clean_data_dir}{i}.node_features.pkl', 'rb') as f:\n",
    "            node_features = pickle.load(f)\n",
    "        \n",
    "        with open(f'{clean_data_dir}{i}.net_features.pkl', 'rb') as f:\n",
    "            edge_features = pickle.load(f)\n",
    "        \n",
    "        with open(f'{clean_data_dir}{i}.congestion.pkl', 'rb') as f:\n",
    "            congestion = pickle.load(f)\n",
    "        \n",
    "        partition = np.load(f'{clean_data_dir}{i}.partition.npy')\n",
    "        \n",
    "        # Preprocess data\n",
    "        node_features = {k: torch.tensor(v).float().to(device) for k, v in node_features.items()}\n",
    "        edge_features = {k: torch.tensor(v).float().to(device) for k, v in edge_features.items()}\n",
    "        \n",
    "        nodes = list(range(len(node_features)))\n",
    "        edges = list(range(len(edge_features)))\n",
    "        hypergraph = Hypergraph(nodes, edges, driver_sink_map, partition, 2)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(node_features, edge_features, hypergraph)\n",
    "        \n",
    "        # Dummy target for illustration (binary labels for each node: 0 for not congested, 1 for congested)\n",
    "        target = torch.tensor(list(congestion.values())).to(device)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()  # Reset gradients after each batch\n",
    "        \n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Print epoch loss\n",
    "    print(f'Epoch [{epoch+1}/10], Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20613864-76d8-4c69-8b62-5367421cbcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
